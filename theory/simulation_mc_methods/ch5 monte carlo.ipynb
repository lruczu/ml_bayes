{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common and antithetic random variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ and $Y$ be random variables with known cdfs, F and G, respectively. Suppose that we need to estimate $l = E[X - Y]$ via simulation. The simplest unbiased estimator of $l$ is $X - Y$. <br>\n",
    "Suppose that $H_{1}$ and $H_{2}$ are real-values monotone functions. The problem can be formulated as follows: <br><br>\n",
    "Within the set of all two-dimensional joint cdfs of (X, Y), find a joint cdf, $F^{*}$, that minimizes $Var(H_{1}(X) - H_{2}(Y))$, subject to X and Y having the prescribed F and G, respectively. <br><br>\n",
    "\n",
    "X and Y need not be independent: <br><br>\n",
    "$Var(X - Y) = Var(X) + Var(Y) - 2Cov(X, Y)$\n",
    "\n",
    "As X and Y have determined variance (specified by F and G), we can try to minimize the variance by maximizing correlation between variables. <br>\n",
    " \n",
    "Inverse transform method gives us: <br>\n",
    "$X = F^{-1}(U_{1}),   U_{1} \\sim U(0, 1)$ <br>\n",
    "$Y = G^{-1}(U_{2}),   U_{2} \\sim U(0, 1)$ <br><br>\n",
    "We say that common random variables are used if $U_{2} = U_{1}$ and antithetic random variables are used if $U_{2} = 1 - U_{1}$. Since both $F^{-1}$ and $G^{-1}$ are nondecreasing functions, in using common random variables, we clearly have <br>\n",
    "$Cov(F^{-1}(U), G^{-1}(U)) \\ge 0$ <br>\n",
    "for $U \\sim U(0, 1)$. Consequently, variance reduction is achieved, in the sense that the estimator $F^{-1}(U) - G^{-1}(U)$ has a smaller variance than the crude Monte Marclo (CMC) estimator X -Y, where X and Y are independent, with cdfs F and G, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main application of antithetic random variables is to estimate $l = E[H(X)]$, <br>\n",
    "where $X \\sim F$ is a random vector with independent components and the sample performance function, H(x), is monotonic in each component of x. \n",
    "\n",
    "An unbiased estimator of  $l = E[H(X)]$ is the CMC estimator, given by <br><br>\n",
    "$\\hat{l} = \\frac{1}{N}\\sum_{k=1}^{N}H(X_{k})$, <br><br>\n",
    "where $X_{1}, .., X_{N}$ is an iid sample from (multidimensional) cdf F. And alternative unbiased estimator of $l$, for even N, is <br><br>\n",
    "$\\hat{l}^{(a)} = \\frac{1}{N}\\sum_{k=1}^{N/2}{H(X_{k}) + H(X_{k}^{(a)})}$\n",
    "\n",
    "Variances can be computed as follows: <br><br>\n",
    "$Var(\\hat{l}) = Var(\\frac{1}{N}\\sum_{k=1}^{N}H(X_{k})) = \\frac{1}{N}Var(H(X_{k}))$ <br>\n",
    "\n",
    "$Var(\\hat{l^{(a)}})= Var(\\frac{1}{N}\\sum_{k=1}^{N/2}{H(X_{k}) + H(X_{k}^{(a)})})$ = $\\frac{1}{N^{2}}\\frac{N}{2}[Var(H(X_{k})) + Var(H(X_{k}^{(a)})) + Cov(X_{k}, X_{k}^{(a)})]$ = $\\frac{1}{2N}[Var(H(X_{k})) + Var(H(X_{k}^{(a)})) + Cov(X_{k}, X_{k}^{(a)})]$ = $\\frac{1}{2}[Var(\\hat{l}) + Var(\\hat{l}) + \\frac{Cov(X_{k}, X_{k}^{(a)})}{N}]$ = $Var(\\hat{l}) + \\frac{Cov(X_{k}, X_{k}^{(a)})}{N}$\n",
    "\n",
    "From computation point of view if hs is a generated sample of length N in the first case and N/2 in the second case, then: <br>\n",
    "$Var(\\hat{l})$ = np.var(hs) / N <br>\n",
    "$Var(\\hat{l^{(a)}})$ = np.var(hs) / (2N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5.1 \n",
    "Consider the integral $\\int_{a}^{b}H(x)dx = (b - a)E[H(X)]$, with $X \\sim U(a, b)$. Let $X_{1}, .., X_{N}$ be a random sample from $U(a, b)$. <br><br> Consider the estimators $\\hat{l} = \\frac{1}{N}\\sum_{i=1}^{N}H(X_{i})$ and $\\hat{l}_{1} = \\frac{1}{2N}\\sum_{i=1}^{N}H(X_{i}) + H(b + a - X_{i}).$ Prove that if H(x) is monotonic in x, then <br><br>\n",
    "$Var(\\hat{l}_{1}) \\le \\frac{1}{2}Var(\\hat{l})$. <br><br>\n",
    "In other words, using antithetic random variables is more accurate than using CMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5.2 \n",
    "Estimate the expected length of the shortest path for the bridge network in example 5.1. Use both CMC and the antithetic estimator. For both cases, take a sample size of N = 100,000. Suppose that the lengths of the links $X_{1}, ..., X_{5}$ are exponentially distributed, with means 1, 1, 0.5, 2, 1.5. Compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "MEANS = [1, 1, 0.5, 2, 1.5]\n",
    "def h(X):\n",
    "    return np.minimum(\n",
    "        np.minimum(\n",
    "            np.minimum(\n",
    "                X[0] + X[3],\n",
    "                X[0] + X[2] + X[4]\n",
    "            ),\n",
    "            X[1] + X[2] + X[3]\n",
    "        ), \n",
    "        X[1] + X[4]\n",
    "    )\n",
    "\n",
    "def inverse_cfd_exp(lambda_, u):\n",
    "    return -lambda_ * np.log(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crude monte carlo\n",
    "X = np.zeros((5, N))\n",
    "for i in range(5):\n",
    "    X[i] = np.random.exponential(MEANS[i], size=N)\n",
    "crude_hs = h(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# antithetic variables\n",
    "X = np.zeros((5, int(N / 2)))\n",
    "X_a = np.zeros((5, int(N / 2)))\n",
    "for i in range(5):\n",
    "    U = np.random.uniform(0, 1, size=int(N / 2))\n",
    "    X[i] = inverse_cfd_exp(MEANS[i], U)\n",
    "    X_a[i] = inverse_cfd_exp(MEANS[i], 1 - U)\n",
    "\n",
    "antithetic_hs = h(X) + h(X_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 50000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crude_hs), len(antithetic_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crude Monte carlo estimate: 1.4934812999306835\n",
      "antithetic random variable estimate: 1.5002795607177157\n"
     ]
    }
   ],
   "source": [
    "print('crude Monte carlo estimate: {}'.format(np.sum(crude_hs) / N))\n",
    "print('antithetic random variable estimate: {}'.format(np.sum(antithetic_hs) / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crude Monte carlo variance: 1.0418068551849287e-05\n",
      "antithetic random variable variance: 5.412574410584615e-06\n"
     ]
    }
   ],
   "source": [
    "print('crude Monte carlo variance: {}'.format(np.var(crude_hs) / N))\n",
    "print('antithetic random variable variance: {}'.format(np.var(antithetic_hs) / (2 * N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control variables\n",
    "Let X be one-dimensional random variable. Let X be an unbiased estimator of $\\mu$ (i.e. E[X] = $\\mu$), to be obtained from the simulation run. A random variable C is called control variable for X if it is correlated with X and its expectation, r, is known (E[C] = r). The control variable C is used to contruct an unbiased estimator of $\\mu$ with a variance smaller than that of X. This estimator, <br><br>\n",
    "$X_{\\alpha} = X - \\alpha(C - r)$\n",
    "\n",
    "The variance of $X_{\\alpha}$ is given by <br><br>\n",
    "$Var(X_{\\alpha}) = Var(X) - 2\\alpha Cov(X, C) + \\alpha^{2}Var(C)$ <br><br>\n",
    "Consequently, the value $\\alpha^{*}$ that minimizes $Var(X_{\\alpha})$ is <br><br>\n",
    "$\\alpha^{*} = \\frac{Cov(X, C)}{Var(C)}$ <br>\n",
    "\n",
    "Typically, $\\alpha^{*}$ is estimated from the corresponding sample covariance and variance. <br>\n",
    "Using $\\alpha^{*}$, we can write the minimal variance as <br><br>\n",
    "$Var(X_{\\alpha^{*}}) = (1 - \\rho_{XC}^{2})Var(X)$, <br>\n",
    "where $\\rho_{XC}$ denotes the correlation coefficient of X and C. The larger $|\\rho_{XC}$ is, the greater is the variance reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5.5\n",
    "Run the stochastic shortest path problem in Example 5.4 and estimate the performance $l = E[H(X)]$ from 100000 independent replications, using the given $(C_{1}, C_{2}, C_{3}, C_{4})$. Compare the results obtained in problem 5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0.5, 2, 1.5]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we take $C = X_{1} + X_{4}$, $E[C] = r = 1 + 2 = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "r = 3\n",
    "X = np.zeros((5, N))\n",
    "for i in range(5):\n",
    "    X[i] = np.random.exponential(MEANS[i], size=N)\n",
    "C = X[0] + X[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_{\\alpha}(X) = H(X) - \\alpha^{*}(C - r)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = h(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21256756632001"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's first estimate alpha\n",
    "alpha = np.cov(hs, C)[0, 0] / np.var(C)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_alpha = hs - alpha * (C - r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control variable method: estimate = 1.5015789577660938\n"
     ]
    }
   ],
   "source": [
    "print('control variable method: estimate = {}'.format(np.mean(hs_alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control variable method: variance = 8.323391925628314e-06\n"
     ]
    }
   ],
   "source": [
    "print('control variable method: variance = {}'.format(np.var(hs_alpha) / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let H be a sample performance function and f probability distribution <br>\n",
    "Very often we are interested in the following quantity: <br> <br>\n",
    "$l = E_{f}[H(X)] = \\int H(x)f(x)dx$\n",
    "\n",
    "\n",
    "Let g be another probability distribution, which satsfies <br>\n",
    "$g(x) = 0 \\rightarrow H(x)f(x) = 0$\n",
    "\n",
    "$l = E_{f}[H(X)] = \\int H(x)f(x)dx = \\int H(x)\\frac{f(x)}{g(x)}g(x)dx = E_{g}[H(X)\\frac{f(X)}{g(X)}]$\n",
    "\n",
    "We can distinguish function $W(x) = \\frac{f(x)}{g(x)}$ called likelihood ratio.\n",
    "\n",
    "If $X_{1}, .., X_{N}$ is a random sample from g, that is $X_{1}, .., X_{N}$ are iid random vectors with density g, then <br><br>\n",
    "$\\hat{l} = \\frac{1}{N}\\sum_{k=1}^{N}H(X_{k})\\frac{f(X_{k})}{g(X_{k})}$\n",
    "\n",
    "\n",
    "#### Variance Minimization Method\n",
    "\n",
    "We want to keep the variance of the estimator low. $\\hat{l}$ is unbiased the estimator of $l$. Let's minimize variance of it, \n",
    "by choosing the best distribution g. \n",
    "\n",
    "$min_{g}$ $Var_{g}(H(X)\\frac{f(X)}{g(X)})$\n",
    "\n",
    "The solution of it: <br><br>\n",
    "$g^{*}(x) = \\frac{|H(x)|f(x)}{\\int|H(x)|f(x)dx}$, if $H(x) \\ge 0$ <br><br>\n",
    "$g^{*}(x) = \\frac{H(x)f(x)}{l}$\n",
    "\n",
    "Let's compute the variance of the optimal distribution: <br><br>\n",
    "$Var_{g^{*}}(\\hat{l}) = Var_{g^{*}}(\\frac{1}{N}\\sum_{k=1}^{N}H(X_{k})\\frac{f(X_{k})}{g^{*}(X_{k})}) = \\frac{1}{N}Var_{g^{*}}(H(X_{1})\\frac{f(X_{1})}{g^{*}(X_{1})}) = \\frac{1}{N}Var_{g^{*}}(l) = 0$, <br> where the last quantity is just a variance of a number. <br><br>\n",
    "The following approach has only some theoretical value, because in order to get an optimal g we need $l$, which we aspire to eventually estimate. <br>\n",
    "Minimization very often is done within some distribution family <br><br>\n",
    "Let $f(\\cdot) = f(\\cdot; u)$, then we focus on g that comes from the same family. We can formalize: <br><br>\n",
    "$min_{v}$ $Var_{v} (H(X) \\frac{f(X; u)}{f(X; v)})$ <br><br>\n",
    "$Var_{v} (H(X) \\frac{f(X; u)}{f(X; v)}) = E_{v}[(H(X) \\frac{f(X; u)}{f(X; v)})^{2}] - E_{v}[(H(X) \\frac{f(X; u)}{f(X; v)}]^{2}$ = $E_{v}[(H(X) \\frac{f(X; u)}{f(X; v)})^{2}] - l^{2}$.\n",
    "So equivalently\n",
    "<br><br>\n",
    "$V(v) = min_{v}$ $E_{v}[(H(X) \\frac{f(X; u)}{f(X; v)})^{2}]$ = $E_{v}[(H(X)W(X; u, v))^{2}]$ = $E_{u}[H(X)^{2}W(X; u, v)]$\n",
    "The above is mostly theoretical derivation. We can try to find the optimal parameter $v$ through sample. Suppose we have $X_{1}, .., X_{N}$ samples from $f(x;u)$, then <br><br>\n",
    "$\\hat{V}(v) = \\frac{1}{N}\\sum_{k=1}^{N}[H^{2}(X_{k})W(X_{k};u, v)]$\n",
    "\n",
    "Assuming we can interchange operators, i.e. <br><br>\n",
    "$\\nabla E_{u}[H^{2}(X)W(X; u, v)] = E_{u}[H^{2}(X)\\nabla W(X; u, v)]$ <br><br>\n",
    "Let's take the derivative <br><br>\n",
    "$\\frac{1}{N}\\sum_{k=1}^{N}[H^{2}(X_{k})\\nabla W(X_{k};u, v)]$ = 0, where <br><br>\n",
    "$\\nabla W(X_{k};u, v) = \\nabla \\frac{f(X;u)}{f(X;v)} = [\\nabla ln f(X;v)] W(X; u, v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entropy Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback-Leibler distance: <br><br>\n",
    "$D(g, h) = E_{g}[ln\\frac{g(X)}{h(X)}] = \\int g(x) ln(\\frac{g(x)}{h(x)})dx = \\int g(x) ln(g(x))dx - \\int g(x)ln(h(x))dx$.\n",
    "\n",
    "We want h that is close to optimal (but theorethical) $g^{*}$. Let's search in some family, h = $f(\\cdot; v)$. The first term  in the KL distance doesn't depend on h, so we can focus on the second term. Moreover, minimization of KL is equivalent to maximization of the second term. So <br>\n",
    "$\\int g^{*}(x) ln(f(x; v)) dx = \\int \\frac{H(x)f(x; u)}{l} ln(f(x; v)) dx \\propto \\int H(x)f(x; u) ln(f(x; v)) dx$ = $E_{u}[H(X)ln f(X; v)]$. Typically, <br><br>\n",
    "$\\nabla E_{u}[H(X)ln f(X; v)] = E_{u}[H(X)\\nabla ln f(X; v)]$, so we can try to find v such that <br><br>\n",
    "$\\frac{1}{N}\\sum_{k=1}^{N}H(X_{k})\\nabla f(X_{k}; v)$ = 0 <br><br>\n",
    "Let w be an arbitrary reference parameter, we can rewrite <br><br>\n",
    "$E_{u}[H(X)ln f(X; v)] = \\int H(x)f(x;u)ln(f(x;v))\\frac{f(x;w)}{f(x;w)}dx$ = $ \\int H(x)W(x;u, w)ln(f(x;v))f(x;w)dx$ = $E_{w}[H(X)W(X; u, w)ln(f(X;v)]$ <br><br>\n",
    "We can estimate $v^{*}$ as the solution of the stochastic program: <br><br>\n",
    "$\\frac{1}{N}\\sum_{k=1}^{N}H(X_{k})W(X_{k};u,w)\\nabla ln(f(X_{k};v)) = 0$, <br><br>\n",
    "where $X_{1}, .., X_{N}$ is a random sample from $f(\\cdot; w)$. In the next iteration found $v^{*}$ can play the role of w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
