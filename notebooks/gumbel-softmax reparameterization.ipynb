{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some utils for using ideas from \"Categorical Reparameterization with Gumbel-Softmax\" (Eric Jang, Shixiang Gu, Ben Poole)\n",
    "https://arxiv.org/abs/1611.01144.  \n",
    "\n",
    "The article deals with the problem of using stochastic categorical node in a neural network. Let's say we have a latent categorical variable, Z $\\in$ $R^{C}$, where C is the number of categories. In normal settings, Z produces one-hot vectors with some probabilities: $\\pi_{1}$, .. $\\pi_{C}$. The problem is that stochastic node cannot be differentiated and it is not clear how to reparameterize it to make it possible. The article proposes using gumbel-softmax.\n",
    "\n",
    "In variational autoencoder setting we have an encoder which models variational distribution. In the case of categorical random variable, it outputs $\\pi_{1}$, $\\pi_{2}$, .., $\\pi_{C}$. No we want to sample from this distribution. The details are in the paper. \n",
    "\n",
    "Fitting the whole model comes down to minimzing the lowerbound of likelihood function or just minimizing the ELBO: <br>\n",
    "ELBO = $E_{Y \\sim q_{\\phi}(y|x)}$[log($p_{\\theta}$(x, y)) - log($q_{\\phi}(y|x)$)] = $E_{Y \\sim q_{\\phi}(y|x)}$[log($p_{\\theta}$(x|y))] - KL($q_{\\phi}(y|x)$||$p_{\\theta}$(y)) = <br>\n",
    "Kullback-Leibler divergence can be written as: <br>\n",
    "KL($q_{\\phi}(y|x)$||$p_{\\theta}$(y)) = $\\sum_{i=1}^{C}$ $\\pi_{i}(log(\\pi_{i}) - log(\\frac{1}{C}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_categorical(pi: tf.Tensor, n_classes: int) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    computes Kullback-Leibler divergence between variational output and \n",
    "    uniform prior on categorical variable\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    pi: is assumed to have rows whose sum is equal to 1 (output of softmax)\n",
    "    n_classes: dimension of categorical variable\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(pi * (tf.log(pi) - np.log(1 / n_classes)), axis=1) \n",
    "\n",
    "def sample_categorical(pi: tf.Tensor, temperature: float) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    samples from categorical distribtion using gumbel-softmax trick. \n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    pi: is assumed to have rows whose sum is equal to 1 (output of softmax)\n",
    "    temperature: the lower it is the samples are closer to one-hot samples. \n",
    "    It must be bigger than 0. \n",
    "    \"\"\"\n",
    "    tensor_shape = K.shape(pi)\n",
    "    batch_size, latent_dim = tensor_shape[0], tensor_shape[1]\n",
    "    \n",
    "    u = tf.random_uniform((batch_size, latent_dim), minval=0, maxval=1)\n",
    "    u = tf.clip_by_value(u, clip_value_min=1e-09, clip_value_max=1-1e-09)\n",
    "    g = -tf.log(-tf.log(u))\n",
    "    \n",
    "    s = tf.exp((tf.log(pi) + g) / temperature)\n",
    "    return s / tf.expand_dims(tf.reduce_sum(s, axis=1), axis=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuck",
   "language": "python",
   "name": "fuck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
